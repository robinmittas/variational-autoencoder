{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zJw9fWjD4HXW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "\n",
        "import torch.utils.data\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torchvision.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "rs1ecJ2p4Mql",
        "outputId": "27de19d1-4ea7-4e8e-866a-5dcc2e12ca8c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0.14.0+cu116'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets import MNIST, CelebA\n",
        "\n",
        "LR_transforms = transforms.Compose([transforms.Resize(14), transforms.ToTensor()])\n",
        "training_data_lr = MNIST(root='./data', transform=LR_transforms, train=True, download=True)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WS-26ptv4TEt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Root directory for the dataset\n",
        "data_root = 'data/celeba'\n",
        "# Spatial size of training images, images are resized to this size.\n",
        "image_size = 64\n",
        "\n",
        "celeba_data = datasets.CelebA(data_root,\n",
        "                              download=True,\n",
        "                              transform=transforms.Compose([\n",
        "                                  transforms.ToTensor(),\n",
        "                                  transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
        "                                                       std=[0.5, 0.5, 0.5])\n",
        "                              ]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lbj75UlmAChD",
        "outputId": "44f157b1-1aa9-496b-ef76-e455af1a950b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(celeba_data[100000][0].permute(1,2,0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "id": "ekLgL0zeAv-b",
        "outputId": "3a496ae5-3e63-42c1-8b57-9f5ce988ab71"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7ff28bac6a00>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import typing\n",
        "from torch import nn\n",
        "from abc import abstractmethod\n",
        "import torch\n",
        "\n",
        "\n",
        "class BaseEncoder(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(BaseEncoder, self).__init__()\n",
        "\n",
        "    def forward(self, x: torch.tensor) -> torch.tensor:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def reparameterize(self, mu: torch.Tensor, logsigma: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Reparameterization trick to sample from N(mu, var): See Appendix\n",
        "        The Encoder returns mean and log(variance). We then sample from Standard Normal Distribution, multiply and add to retrieve\n",
        "        exp(0.5*log_sigma)*N(0,1) + mu          ~ N(mu, sigma)\n",
        "        which then behaves like ~ N(mu, sigma) and thus we have sampled from latent space.\n",
        "        :param mu: (torch.Tensor) Mean of the latent Gaussian [B x D]\n",
        "        :param logsigma: (torch.Tensor) Standard deviation of the latent Gaussian [B x D]\n",
        "        :return: (torch.Tensor) [B x D] ~ N(mu, sigma) for each element in batch\n",
        "        \"\"\"\n",
        "        std = torch.exp(torch.mul(0.5, logsigma))\n",
        "        # retrieve eps ~ N(0, sigma)\n",
        "        eps = torch.randn_like(std)\n",
        "        return eps * std + mu\n",
        "\n",
        "\n",
        "class BaseDecoder(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(BaseDecoder, self).__init__()\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "class BaseVarAutoencoder(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(BaseVarAutoencoder, self).__init__()\n",
        "\n",
        "    def sample(self, batch_size: int, current_device: int, **kwargs) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        this functions samples with given batch size from Standard Normal Distribution N(0,1) with latent dimension\n",
        "        :param batch_size: how many samples to draw\n",
        "        :param current_device: devide\n",
        "        :param kwargs:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        standard_normal_samples = torch.randn(batch_size, self.latent_dimension)\n",
        "        decoded = self.decoder(standard_normal_samples.to(current_device))\n",
        "        return decoded\n",
        "\n",
        "    def generate(self, x: torch.Tensor, **kwargs) -> torch.Tensor:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @abstractmethod\n",
        "    def forward(self, *inputs: torch.Tensor):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def loss(self, *inputs: typing.Any, **kwargs) -> dict:\n",
        "        pass\n",
        "\n",
        "\n",
        "class BaseConvBlock(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels: int,\n",
        "                 out_channels: int,\n",
        "                 kernel_size,\n",
        "                 stride,\n",
        "                 padding: int,\n",
        "                 max_pool: bool,\n",
        "                 **kwargs):\n",
        "\n",
        "        super(BaseConvBlock, self).__init__()\n",
        "        self.conv_layer = nn.Conv2d(in_channels=in_channels,\n",
        "                                    out_channels=out_channels,\n",
        "                                    kernel_size=kernel_size,\n",
        "                                    stride=stride,\n",
        "                                    padding=padding)\n",
        "\n",
        "        if max_pool:\n",
        "            self.max_pooling = nn.MaxPool2d(kernel_size=kwargs[\"kernel_max_pool\"], stride=kwargs[\"stride_max_pool\"])\n",
        "        else:\n",
        "            # if we dont want to use maxpool, set kernel and stride to 1\n",
        "            self.max_pooling = nn.MaxPool2d(kernel_size=1, stride=1)\n",
        "        self.batch_norm = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        out = self.conv_layer(x)\n",
        "        out = self.max_pooling(out)\n",
        "        out = self.batch_norm(out)\n",
        "        return self.relu(out)\n",
        "\n",
        "\n",
        "class BaseTransposeConvBlock(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels: int,\n",
        "                 out_channels: int,\n",
        "                 kernel_size,\n",
        "                 stride,\n",
        "                 padding: int,\n",
        "                 upsample: bool,\n",
        "                 **kwargs):\n",
        "\n",
        "        super(BaseTransposeConvBlock, self).__init__()\n",
        "        self.transpose_conv_layer = nn.ConvTranspose2d(in_channels=in_channels,\n",
        "                                                       out_channels=out_channels,\n",
        "                                                       kernel_size=kernel_size,\n",
        "                                                       stride=stride,\n",
        "                                                       padding=padding,\n",
        "                                                       output_padding=1)\n",
        "\n",
        "        if upsample:\n",
        "            self.upsample = nn.Upsample(scale_factor=kwargs[\"scale_factor\"])\n",
        "        else:\n",
        "            # if we dont want to use upsample, set kernel and stride to 1\n",
        "            self.upsample = nn.Upsample(scale_factor=1)\n",
        "        self.batch_norm = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        out = self.transpose_conv_layer(x)\n",
        "        out = self.upsample(out)\n",
        "        out = self.batch_norm(out)\n",
        "        return self.relu(out)\n",
        "\n"
      ],
      "metadata": {
        "id": "LY1BuJvvCqBP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch_lightning"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tPnfxeXR3cE",
        "outputId": "3245705d-7d71-4eea-a7d2-175fffbb654c"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytorch_lightning\n",
            "  Downloading pytorch_lightning-1.8.5-py3-none-any.whl (800 kB)\n",
            "\u001b[K     |████████████████████████████████| 800 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning) (2022.11.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning) (21.3)\n",
            "Collecting lightning-utilities!=0.4.0,>=0.3.0\n",
            "  Downloading lightning_utilities-0.4.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning) (6.0)\n",
            "Collecting tensorboardX>=2.2\n",
            "  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 62.2 MB/s \n",
            "\u001b[?25hCollecting torchmetrics>=0.7.0\n",
            "  Downloading torchmetrics-0.11.0-py3-none-any.whl (512 kB)\n",
            "\u001b[K     |████████████████████████████████| 512 kB 49.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning) (1.13.0+cu116)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning) (4.4.0)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning) (4.64.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from fsspec[http]>2021.06.0->pytorch_lightning) (2.23.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.8/dist-packages (from fsspec[http]>2021.06.0->pytorch_lightning) (3.8.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (22.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (6.0.3)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (2.1.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.8.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (4.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.3.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=17.0->pytorch_lightning) (3.0.9)\n",
            "Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /usr/local/lib/python3.8/dist-packages (from tensorboardX>=2.2->pytorch_lightning) (3.19.6)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.8/dist-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (2022.9.24)\n",
            "Installing collected packages: torchmetrics, tensorboardX, lightning-utilities, pytorch-lightning\n",
            "Successfully installed lightning-utilities-0.4.2 pytorch-lightning-1.8.5 tensorboardX-2.5.1 torchmetrics-0.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "train_loader = DataLoader(celeba_data, batch_size=64)"
      ],
      "metadata": {
        "id": "MqidcP6MSRND"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(train_loader))[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wz-rwuwLrjq8",
        "outputId": "6c9c4a1d-975a-4fe9-c839-c251828be4f7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([64, 3, 218, 178])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(celeba_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBEomzLer3e1",
        "outputId": "20cf5630-95f6-488f-a7f2-e7578c5a5d80"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "162770"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_lightning import LightningDataModule\n",
        "\n",
        "class MyCelebA(CelebA):\n",
        "    \"\"\"\n",
        "    A work-around to address issues with pytorch's celebA dataset class.\n",
        "    \n",
        "    Download and Extract\n",
        "    URL : https://drive.google.com/file/d/1m8-EBPgi5MRubrm6iQjafK2QMHDBMSfJ/view?usp=sharing\n",
        "    \"\"\"\n",
        "    \n",
        "    def _check_integrity(self) -> bool:\n",
        "        return True\n",
        "\n",
        "\n",
        "  \n",
        "class VAEDataset(LightningDataModule):\n",
        "    \"\"\"\n",
        "    PyTorch Lightning data module \n",
        "\n",
        "    Args:\n",
        "        data_dir: root directory of your dataset.\n",
        "        train_batch_size: the batch size to use during training.\n",
        "        val_batch_size: the batch size to use during validation.\n",
        "        patch_size: the size of the crop to take from the original images.\n",
        "        num_workers: the number of parallel workers to create to load data\n",
        "            items (see PyTorch's Dataloader documentation for more details).\n",
        "        pin_memory: whether prepared items should be loaded into pinned memory\n",
        "            or not. This can improve performance on GPUs.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        data_path: str,\n",
        "        train_batch_size: int = 8,\n",
        "        val_batch_size: int = 8,\n",
        "        patch_size: Union[int, Sequence[int]] = (256, 256),\n",
        "        num_workers: int = 0,\n",
        "        pin_memory: bool = False,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.data_dir = data_path\n",
        "        self.train_batch_size = train_batch_size\n",
        "        self.val_batch_size = val_batch_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_workers = num_workers\n",
        "        self.pin_memory = pin_memory\n",
        "\n",
        "    def setup(self, stage: Optional[str] = None) -> None:\n",
        "#       =========================  OxfordPets Dataset  =========================\n",
        "            \n",
        "#         train_transforms = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
        "#                                               transforms.CenterCrop(self.patch_size),\n",
        "# #                                               transforms.Resize(self.patch_size),\n",
        "#                                               transforms.ToTensor(),\n",
        "#                                                 transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
        "        \n",
        "#         val_transforms = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
        "#                                             transforms.CenterCrop(self.patch_size),\n",
        "# #                                             transforms.Resize(self.patch_size),\n",
        "#                                             transforms.ToTensor(),\n",
        "#                                               transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
        "\n",
        "#         self.train_dataset = OxfordPets(\n",
        "#             self.data_dir,\n",
        "#             split='train',\n",
        "#             transform=train_transforms,\n",
        "#         )\n",
        "        \n",
        "#         self.val_dataset = OxfordPets(\n",
        "#             self.data_dir,\n",
        "#             split='val',\n",
        "#             transform=val_transforms,\n",
        "#         )\n",
        "        \n",
        "#       =========================  CelebA Dataset  =========================\n",
        "    \n",
        "        train_transforms = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
        "                                              transforms.CenterCrop(148),\n",
        "                                              transforms.Resize(self.patch_size),\n",
        "                                              transforms.ToTensor(),])\n",
        "        \n",
        "        val_transforms = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
        "                                            transforms.CenterCrop(148),\n",
        "                                            transforms.Resize(self.patch_size),\n",
        "                                            transforms.ToTensor(),])\n",
        "        ################## comment here\n",
        "        self.train_dataset = MNIST(self.data_dir, train=True, transform=transforms.ToTensor())\n",
        "        self.val_dataset = MNIST(self.data_dir, train=False, transform=transforms.ToTensor())\n",
        "\n",
        "        self.train_dataset = MyCelebA(\n",
        "            self.data_dir,\n",
        "            split='train',\n",
        "            transform=train_transforms,\n",
        "            download=False,\n",
        "        )\n",
        "        \n",
        "        # Replace CelebA with your dataset\n",
        "        self.val_dataset = MyCelebA(\n",
        "            self.data_dir,\n",
        "            split='test',\n",
        "            transform=val_transforms,\n",
        "            download=False,\n",
        "        )\n",
        "        ########################\n",
        "#       ===============================================================\n",
        "        \n",
        "    def train_dataloader(self) -> DataLoader:\n",
        "        return DataLoader(\n",
        "            self.train_dataset,\n",
        "            batch_size=self.train_batch_size,\n",
        "            num_workers=self.num_workers,\n",
        "            shuffle=True,\n",
        "            pin_memory=self.pin_memory,\n",
        "        )\n",
        "\n",
        "    def val_dataloader(self) -> Union[DataLoader, List[DataLoader]]:\n",
        "        return DataLoader(\n",
        "            self.val_dataset,\n",
        "            batch_size=self.val_batch_size,\n",
        "            num_workers=self.num_workers,\n",
        "            shuffle=False,\n",
        "            pin_memory=self.pin_memory,\n",
        "        )\n",
        "    \n",
        "    def test_dataloader(self) -> Union[DataLoader, List[DataLoader]]:\n",
        "        return DataLoader(\n",
        "            self.val_dataset,\n",
        "            batch_size=144,\n",
        "            num_workers=self.num_workers,\n",
        "            shuffle=True,\n",
        "            pin_memory=self.pin_memory,\n",
        "        )\n",
        "     "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "zl9aUlMBRfmp",
        "outputId": "d0ddd22c-82b6-4f9f-b3aa-75eacc826023"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-ca2d5a94d0d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mVAEDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLightningDataModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \"\"\"\n\u001b[1;32m     18\u001b[0m     \u001b[0mPyTorch\u001b[0m \u001b[0mLightning\u001b[0m \u001b[0mdata\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-46-ca2d5a94d0d8>\u001b[0m in \u001b[0;36mVAEDataset\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mtrain_batch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mval_batch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mpatch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mnum_workers\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Union' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional\n",
        "\n",
        "class VarBayesianEncoder(BaseEncoder):\n",
        "    def __init__(self,\n",
        "                 in_channels: int,\n",
        "                 hidden_dimensions,\n",
        "                 latent_dimension: int,\n",
        "                 kernel_size,\n",
        "                 stride,\n",
        "                 padding: int,\n",
        "                 max_pool,\n",
        "                 linear_layer_dimension: int):\n",
        "        \"\"\"\n",
        "        Variational Bayesian Encoder\n",
        "        :param in_channels: Define in Channels of Input image (e.g. for [1,28,28] --> 1)\n",
        "        :param hidden_dimensions: define hidden dimensions for Convolutional Blocks: list of ints [32,64,128..]\n",
        "        :param latent_dimension: The dimension of the latent space of which we will sample\n",
        "        :param kernel_size: Kernel size of Conv Layers (e.g. (2,2) or (3,3)\n",
        "        :param stride: Stride for Conv Layers (tuple of ints)\n",
        "        :param padding: Padding for both [H,W]\n",
        "        :param max_pool: list of boolean (needs to have same length as @hidden_dimensions). Defines wether to use Max Pool in Conv Block\n",
        "        :param linear_layer_dimension: The input dimension for last linear layer (e.g. the output dimension of (H or W) of last Conv Block)\n",
        "        \"\"\"\n",
        "\n",
        "        super(VarBayesianEncoder, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.latent_dimension = latent_dimension\n",
        "\n",
        "        conv_blocks_encoder = []\n",
        "        for idx, hidden_dim in enumerate(hidden_dimensions):\n",
        "            # Build Encoder\n",
        "            conv_blocks_encoder.append(BaseConvBlock(in_channels=in_channels,\n",
        "                                                     out_channels=hidden_dim,\n",
        "                                                     kernel_size=kernel_size,\n",
        "                                                     stride=stride,\n",
        "                                                     padding=padding,\n",
        "                                                     max_pool=max_pool[idx]))\n",
        "            in_channels = hidden_dim\n",
        "\n",
        "        self.encoder = nn.Sequential(*conv_blocks_encoder)\n",
        "        # now add two dense layer to get mu and sigma from the latent space\n",
        "        self.linear1 = nn.Linear(hidden_dimensions[-1] * linear_layer_dimension**2, latent_dimension)\n",
        "        self.linear2 = nn.Linear(hidden_dimensions[-1] * linear_layer_dimension**2, latent_dimension)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Forward Method for input batch-image\n",
        "        :param x: batch\n",
        "        :return: mu, log(sigma) and sample from N(mu, sigma)\n",
        "        \"\"\"\n",
        "        out = self.encoder(x)\n",
        "        # get out tensor into [B, x] shape for linear Layers\n",
        "        out = torch.flatten(out, start_dim=1)\n",
        "        mu = self.linear1(out)\n",
        "        logsigma = self.linear2(out)\n",
        "        return [self.reparameterize(mu, logsigma), mu, logsigma]\n",
        "\n",
        "    def check_forward_shape(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        With this function you can check the dimensions/ shape of yout input tensor after the last convolutional block\n",
        "        in order to define the input dimensions of the linear layers. For that first create an instance of class with\n",
        "        dummy values.\n",
        "        :param x: torch tensor of shape [B, C, H, W]\n",
        "        :return: Tensor after conv layers (get shape by .size() or .shape\n",
        "        \"\"\"\n",
        "        return self.encoder(x)\n",
        "\n",
        "\n",
        "\n",
        "class VarBayesianDecoder(BaseDecoder):\n",
        "    def __init__(self,\n",
        "                 in_channels: int,\n",
        "                 hidden_dimensions,\n",
        "                 latent_dimension: int,\n",
        "                 kernel_size,\n",
        "                 stride,\n",
        "                 padding: int,\n",
        "                 upsample,\n",
        "                 linear_layer_dimension: int):\n",
        "        \"\"\"\n",
        "        Variational Bayesian Decoder\n",
        "        :param in_channels: Define in Channels of initial Input image (e.g. for [1,28,28] --> 1)\n",
        "        :param hidden_dimensions: define hidden dimensions for Transp-Convolutional Blocks: list of ints [32,64,128..]\n",
        "        :param latent_dimension: The dimension of the latent space of which we will sample\n",
        "        :param kernel_size: Kernel size of Transp-Conv Layers (e.g. (2,2) or (3,3)\n",
        "        :param stride: Stride for Transp-Conv Layers (tuple of ints)\n",
        "        :param padding: Padding for both [H,W]\n",
        "        :param upsample: list of boolean (needs to have same length as @hidden_dimensions). Defines wether to use Upsample in Transp-Conv Block\n",
        "        :param linear_layer_dimension: The input dimension for last linear layer (e.g. the output dimension of (H or W) of last Conv Block)\n",
        "        \"\"\"\n",
        "        super(VarBayesianDecoder, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.latent_dimension = latent_dimension\n",
        "\n",
        "        # we use same hidden dimension as encoder ([32,64,128...]) but as we now want to transpose/ decode, we reverse order\n",
        "        hidden_dimensions.reverse()\n",
        "\n",
        "        # first add linear layer to reshape sampled vector from latent space\n",
        "        self.linear_dim = linear_layer_dimension\n",
        "        self.hidden_dim_first = hidden_dimensions[0]\n",
        "        self.linear = nn.Linear(latent_dimension, self.linear_dim**2 * self.hidden_dim_first)\n",
        "\n",
        "        conv_blocks_decoder = []\n",
        "        for idx, hidden_dim in enumerate(hidden_dimensions):\n",
        "            if idx != len(hidden_dimensions)-1:\n",
        "                in_channels = hidden_dim\n",
        "                out_channels = hidden_dimensions[idx + 1]\n",
        "            else:\n",
        "                in_channels = hidden_dim\n",
        "                out_channels = self.in_channels\n",
        "            # Build Decoder\n",
        "            conv_blocks_decoder.append(BaseTransposeConvBlock(in_channels=in_channels,#hidden_dim,\n",
        "                                                              out_channels=out_channels,#hidden_dimensions[idx + 1],\n",
        "                                                              kernel_size=kernel_size,\n",
        "                                                              stride=stride,\n",
        "                                                              padding=padding,\n",
        "                                                              upsample=upsample[idx]))\n",
        "        self.decoder = nn.Sequential(*conv_blocks_decoder)\n",
        "        # add one last layer as heigt, weight of image is bigger then initally\n",
        "        self.final_layer = nn.Conv2d(in_channels=self.in_channels,\n",
        "                                     out_channels=self.in_channels,\n",
        "                                     kernel_size=(6, 6),\n",
        "                                     stride=(1, 1),\n",
        "                                     padding=0)\n",
        "        #self.tanh = nn.Tanh()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Forward method of Decoder\n",
        "        :param x: Batch of samples from Latent Space\n",
        "        :return: Batch of reconstructed Images\n",
        "        \"\"\"\n",
        "        out = self.linear(x)\n",
        "        # now reshape tensor of shape [B, hidden_dimensions[0] * linear_layer_dimension**2]\n",
        "        out = out.view(-1, self.hidden_dim_first, self.linear_dim, self.linear_dim)\n",
        "        out = self.decoder(out)\n",
        "        out = self.final_layer(out)\n",
        "        return self.sigmoid(out)\n",
        "\n",
        "\n",
        "class BetaVAE(BaseVarAutoencoder):\n",
        "    def __init__(self,\n",
        "                 in_channels: int,\n",
        "                 hidden_dimensions,\n",
        "                 latent_dimension: int,\n",
        "                 kernel_size,\n",
        "                 stride,\n",
        "                 padding: int,\n",
        "                 max_pool,\n",
        "                 linear_layer_dimension: int):\n",
        "        super(BetaVAE, self).__init__()\n",
        "\n",
        "        self.latent_dimension = latent_dimension\n",
        "        self.encoder = VarBayesianEncoder(in_channels=in_channels,\n",
        "                                          hidden_dimensions=hidden_dimensions,\n",
        "                                          latent_dimension=latent_dimension,\n",
        "                                          kernel_size=kernel_size,\n",
        "                                          stride=stride,\n",
        "                                          padding=padding,\n",
        "                                          max_pool=max_pool,\n",
        "                                          linear_layer_dimension=linear_layer_dimension)\n",
        "\n",
        "        self.decoder = VarBayesianDecoder(in_channels=in_channels,\n",
        "                                          hidden_dimensions=hidden_dimensions,\n",
        "                                          latent_dimension=latent_dimension,\n",
        "                                          kernel_size=kernel_size,\n",
        "                                          stride=stride,\n",
        "                                          padding=padding,\n",
        "                                          upsample=max_pool,\n",
        "                                          linear_layer_dimension=linear_layer_dimension)\n",
        "\n",
        "    def forward(self, inputs: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Forward Method for Variational Autoencoder\n",
        "        :param inputs: Takes a batch of images of inital size as an input [B,C,H,W]\n",
        "        :return: [reconstruction, inputs, sample from latent space, mu, log_sigma]\n",
        "        \"\"\"\n",
        "        encode = self.encoder(inputs)\n",
        "        decode = self.decoder(encode[0])\n",
        "        return [decode, inputs] + encode\n",
        "\n",
        "    def loss(self, inputs: list, **kwargs) -> dict:\n",
        "        \"\"\"\n",
        "        The loss function for Variational Bayesian AE\n",
        "        :param inputs: [reconstruction, orig_input, latent_sample, mu, log_sigma], which is exactly the output of forward pass\n",
        "        :param kwargs: We need following two parameters: \"KL_divergence_weight\" and which MSE Loss to use: \"mse_reduction\"\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        reconstruction, orig_input, latent_sample, mu, log_sigma = inputs[0], inputs[1], inputs[2], inputs[3], inputs[4]\n",
        "\n",
        "        reconstruction_loss = nn.functional.binary_cross_entropy(reconstruction, orig_input, reduction=kwargs[\"mse_reduction\"])\n",
        "\n",
        "        reconstruction_loss = reconstruction_loss / kwargs[\"batch_size\"]\n",
        "\n",
        "        # for derivation of KL Term of two Std. Normals, see Appendix TODO!\n",
        "        # KL_divergence_loss = torch.mean(-0.5 * torch.sum(1 + log_sigma - mu ** 2 - log_sigma.exp(), dim=1), dim=0)\n",
        "        KL_divergence_loss = -0.5 * torch.mean(1 + log_sigma - mu ** 2 - log_sigma.exp(), dim=0)\n",
        "        KL_divergence_loss = torch.sum(KL_divergence_loss)\n",
        "\n",
        "        # Add a weight to KL divergence term as the loss otherwise is too much dominated by this term!\n",
        "        # For Validation we set this to 1\n",
        "        KL_divergence_weight = kwargs[\"KL_divergence_weight\"]\n",
        "        kld_factor = self.linear_scale_kld_factor(kwargs[\"current_train_step\"], \n",
        "                                                  kwargs[\"first_k_train_steps\"], \n",
        "                                                  0, \n",
        "                                                  1) if kwargs[\"scale_kld\"] else 1\n",
        "\n",
        "        total_loss = reconstruction_loss + kld_factor * (KL_divergence_weight * KL_divergence_loss)\n",
        "\n",
        "        return {\"total_loss\": total_loss,\n",
        "                \"kl_divergence_loss\": KL_divergence_loss,\n",
        "                \"reconstruction_loss\": reconstruction_loss}\n",
        "\n",
        "    def linear_scale_kld_factor(self, current_training_step: int, first_k_train_steps: int, min_value=0, max_value=1):\n",
        "      if first_k_train_steps==0:\n",
        "        return max_value\n",
        "      delta = max_value - min_value\n",
        "      scale_factor = min(min_value + delta * current_training_step/ first_k_train_steps, max_value)\n",
        "      return\n",
        "\n",
        "      "
      ],
      "metadata": {
        "id": "KXqAIyQ-DaZy"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch_lightning"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3mkxVn1mvuhY",
        "outputId": "ea3647b4-d76b-4139-f4a3-a8bd031b19c5"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytorch_lightning\n",
            "  Downloading pytorch_lightning-1.8.5.post0-py3-none-any.whl (800 kB)\n",
            "\u001b[K     |████████████████████████████████| 800 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning) (1.13.0+cu116)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning) (4.4.0)\n",
            "Collecting torchmetrics>=0.7.0\n",
            "  Downloading torchmetrics-0.11.0-py3-none-any.whl (512 kB)\n",
            "\u001b[K     |████████████████████████████████| 512 kB 4.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning) (2022.11.0)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning) (1.21.6)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning) (21.3)\n",
            "Collecting tensorboardX>=2.2\n",
            "  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 45.5 MB/s \n",
            "\u001b[?25hCollecting lightning-utilities!=0.4.0,>=0.3.0\n",
            "  Downloading lightning_utilities-0.4.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from fsspec[http]>2021.06.0->pytorch_lightning) (2.23.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.8/dist-packages (from fsspec[http]>2021.06.0->pytorch_lightning) (3.8.3)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (2.1.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (6.0.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.3.3)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (4.0.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.8.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (22.1.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=17.0->pytorch_lightning) (3.0.9)\n",
            "Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /usr/local/lib/python3.8/dist-packages (from tensorboardX>=2.2->pytorch_lightning) (3.19.6)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.8/dist-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (2022.12.7)\n",
            "Installing collected packages: torchmetrics, tensorboardX, lightning-utilities, pytorch-lightning\n",
            "Successfully installed lightning-utilities-0.4.2 pytorch-lightning-1.8.5.post0 tensorboardX-2.5.1 torchmetrics-0.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt; plt.rcParams['figure.dpi'] = 200\n",
        "import numpy as np\n",
        "from mpl_toolkits.axes_grid1 import ImageGrid\n",
        "import torchvision.utils as vutils\n",
        "\n",
        "\n",
        "def interpolate_2_images(model,\n",
        "                         test_data_loader,\n",
        "                         label_1: int,\n",
        "                         label_2: int,\n",
        "                         device,\n",
        "                         path,\n",
        "                         n=12):\n",
        "    \"\"\"\n",
        "    This function interpolates between two images: Both images are encoded, then the latent representations are interpolated with n steps\n",
        "    :param model: VAE model (nn.Module subclassed)\n",
        "    :param test_data_loader: Test Data Loader\n",
        "    :param label_1: labels, e.g. 1 or 2\n",
        "    :param label_2: labels\n",
        "    :param device: device (gpu/ cpu)\n",
        "    :param path: path to save image\n",
        "    :param n: how many images we interpolate between those two\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    x, y = next(iter(test_data_loader))  # hack to grab a batch\n",
        "    x_1 = x[y == label_1][1].unsqueeze(dim=0)\n",
        "    x_2 = x[y == label_2][1].unsqueeze(dim=0)\n",
        "    z_1 = model.encoder(x_1.to(device))[0]\n",
        "    z_2 = model.encoder(x_2.to(device))[0]\n",
        "    z = torch.stack([z_1 + (z_2 - z_1) * t for t in np.linspace(0, 1, n)])\n",
        "    interpolate_list = model.decoder(z)\n",
        "\n",
        "    vutils.save_image(interpolate_list.cpu().data,\n",
        "                      path,\n",
        "                      normalize=True,\n",
        "                      nrow=n)\n",
        "\n",
        "def plot_from_distribution(model,\n",
        "                           test_data_loader,\n",
        "                           nrow_ncols: (int, int),\n",
        "                           label: int,\n",
        "                           noise: float = 0.0):\n",
        "    x, y = next(iter(test_data_loader))\n",
        "    x = x[y == label][1]\n",
        "\n",
        "    fig = plt.figure(figsize=(4., 4.))\n",
        "    grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
        "                     nrows_ncols=nrow_ncols)\n",
        "\n",
        "    x_1_latent = model.model.encoder(x.unsqueeze(dim=1))\n",
        "    for ax in grid:\n",
        "        standard_gauss = torch.randn_like(x_1_latent[1])\n",
        "        sample_latent = x_1_latent[1] + standard_gauss * torch.exp(0.5 * x_1_latent[2])\n",
        "\n",
        "        sample_latent += noise * torch.randn_like(x_1_latent[1])\n",
        "\n",
        "        sample_construct = model.model.decoder(sample_latent)[0]\n",
        "\n",
        "        #orig_z = model.model.decoder(x_1_latent[0])[0]\n",
        "        # ax.imshow(orig_z.permute(1, 2, 0).detach().numpy())\n",
        "        ax.imshow(sample_construct.permute(1, 2, 0).detach().numpy())\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_2d_latent_space(autoencoder, r0=(-1, 1), r1=(-1, 1), n=50, input_dimension=[1, 28, 28]):\n",
        "    \"\"\"\n",
        "    :param autoencoder:\n",
        "    :param r0: borders for drawing from latent space\n",
        "    :param r1: borders for drawing from latent space\n",
        "    :param n: nxn grid\n",
        "    :param input_dimension: input dimension of one image\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    latent_space = torch.empty(n, n, input_dimension[0], input_dimension[1], input_dimension[2])\n",
        "    for i, y in enumerate(np.linspace(*r1, n)):\n",
        "        for j, x in enumerate(np.linspace(*r0, n)):\n",
        "            z = torch.Tensor([[x, y]])#.to(device)\n",
        "            latent_space[i, j] = autoencoder.decoder(z)\n",
        "    latent_space = latent_space.cpu().data.view(-1, input_dimension[0], input_dimension[1], input_dimension[2])\n",
        "    vutils.save_image(latent_space,\n",
        "                      \"./plots/2d_latent_space.png\",\n",
        "                      normalize=True,\n",
        "                      nrow=n)\n",
        "    return latent_space"
      ],
      "metadata": {
        "id": "VKNKNG111JIR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision import transforms, utils\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
        "import yaml\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "\n",
        "\n",
        "class VAETrainer(pl.LightningModule):\n",
        "    def __init__(self, model: BaseVarAutoencoder, params):\n",
        "        super(VAETrainer, self).__init__()\n",
        "        self.model = model\n",
        "        self.params = params\n",
        "        self.save_hyperparameters()\n",
        "        self.current_device = params[\"devices\"]\n",
        "        self.current_training_step = 0\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.params[\"learning_rate\"])\n",
        "        return optimizer\n",
        "\n",
        "    def training_step(self, train_batch, batch_idx):\n",
        "        self.current_training_step += 1\n",
        "        x, y = train_batch\n",
        "        self.current_device = x.device\n",
        "        output = self.forward(x)\n",
        "        loss = self.model.loss(output,\n",
        "                               current_train_step=self.current_training_step,\n",
        "                               **self.params)\n",
        "\n",
        "        self.log_dict({f\"train_{loss_key}\": loss_val.item() for loss_key, loss_val in loss.items()})\n",
        "\n",
        "        return loss[\"total_loss\"]\n",
        "\n",
        "    def validation_step(self, val_batch, batch_idx):\n",
        "        x, y = val_batch\n",
        "        self.current_device = x.device\n",
        "        output = self.forward(x)\n",
        "        loss = self.model.loss(output,\n",
        "                               mse_reduction=self.params[\"mse_reduction\"],\n",
        "                               KL_divergence_weight=1) # for validation step set KLD weight always to 1\n",
        "\n",
        "        self.log_dict({f\"valid_{loss_key}\": loss_val.item() for loss_key, loss_val in loss.items()})\n",
        "\n",
        "        # Log reconstructed validation images!\n",
        "        tensorboard = self.logger.experiment\n",
        "        img_grid = utils.make_grid(output[0])\n",
        "        tensorboard.add_image(f'Reconstructed Images {self.current_epoch}', img_grid)\n",
        "        # It seems like validation batches are shuffled\n",
        "        img_grid = utils.make_grid(output[1])\n",
        "        tensorboard.add_image(f'Original Images {self.current_epoch}', img_grid)\n",
        "\n",
        "        return loss[\"total_loss\"]\n",
        "\n",
        "    def on_validation_epoch_end(self) -> None:\n",
        "        \"\"\"\n",
        "        At the end of epoch we want to Sample from Latent Space and log these\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        sampled = self.model.sample(144, self.current_device)\n",
        "        img_grid = utils.make_grid(sampled)\n",
        "        tensorboard = self.logger.experiment\n",
        "        tensorboard.add_image(f'Sampled Images {self.current_epoch}', img_grid)\n",
        "\n",
        "    def on_train_end(self) -> None:\n",
        "        #interpolate_2_images(self.model,\n",
        "        #                     test_loader,\n",
        "        #                     label_1=1,\n",
        "        #                     label_2=3,\n",
        "        #                     n=12,\n",
        "        #                     device=self.current_device,\n",
        "        #                     path=self.params[\"plot_2_interpolate_dir\"])\n",
        "        sampled = self.model.sample(144, self.current_device)\n",
        "        img_grid = utils.make_grid(sampled, nrow=12)\n",
        "        vutils.save_image(img_grid.cpu().data,\n",
        "                          self.params[\"plot_sample\"],\n",
        "                          normalize=True,\n",
        "                          nrow=12)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    with open(\"configs/beta_vae.yaml\", encoding='utf8') as conf:\n",
        "        config = yaml.load(conf, Loader=yaml.FullLoader)\n",
        "        conf.close()\n",
        "    # use MNIST Dataset and load training and test data\n",
        "    #training_data = MNIST(root='./data', transform=transforms.ToTensor(), train=True, download=True)\n",
        "\n",
        "    #test_loader = torch.utils.data.DataLoader(\n",
        "    #    MNIST(root='./data', transform=transforms.ToTensor(), train=False, download=True),\n",
        "    #    batch_size=128,\n",
        "    #    shuffle=True)\n",
        "\n",
        "    train_size = int(config[\"train_valid_split\"] * len(celeba_data))\n",
        "    val_size = len(celeba_data) - train_size\n",
        "    train_set, val_set = torch.utils.data.random_split(celeba_data, [train_size, val_size])\n",
        "    \n",
        "    # Load data into torch Dataloader\n",
        "    train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
        "    val_loader = DataLoader(val_set, batch_size=64, shuffle=True)\n",
        "\n",
        "    #train_loader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True)\n",
        "    #val_loader = torch.utils.data.DataLoader(val_set, batch_size=64, shuffle=True)\n",
        "\n",
        "    #vae = SigmaVAE(in_channels=config[\"input_image_size\"][0],\n",
        "    #               hidden_dimensions=config[\"hidden_dimensions\"],\n",
        "    #               latent_dimension=config[\"latent_dimension\"],\n",
        "    #               kernel_size=config[\"kernel_size\"],\n",
        "    #               stride=config[\"stride\"],\n",
        "    #               padding=config[\"padding\"],\n",
        "    #               max_pool=config[\"max_pool\"],\n",
        "    #               linear_layer_dimension=config[\"linear_layer_dimension\"])\n",
        "\n",
        "    vae = BetaVAE(in_channels=config[\"input_image_size\"][0],\n",
        "                        hidden_dimensions=config[\"hidden_dimensions\"],\n",
        "                        latent_dimension= config[\"latent_dimension\"],\n",
        "                        kernel_size=config[\"kernel_size\"],\n",
        "                        stride=config[\"stride\"],\n",
        "                        padding=config[\"padding\"],\n",
        "                        max_pool=config[\"max_pool\"],\n",
        "                        linear_layer_dimension=config[\"linear_layer_dimension\"])\n",
        "\n",
        "    #vae = LinearVAE(input_dimension=config[\"input_image_size\"],\n",
        "    #                hidden_dimensions=config[\"hidden_dimensions\"],\n",
        "    #                latent_dim=config[\"latent_dim\"])\n",
        "\n",
        "    tb_logger = TensorBoardLogger(save_dir=config['logging_dir'],\n",
        "                                  name=config['logging_name'])\n",
        "\n",
        "    # config contains further hyperparameters (LR/ KLD Weight/ MSE Reduction)\n",
        "    model = VAETrainer(model=vae, params=config)\n",
        "    trainer = pl.Trainer(callbacks=[EarlyStopping(monitor=\"valid_total_loss\", mode=\"min\")],\n",
        "                         logger=tb_logger,\n",
        "                         max_epochs=config[\"epochs\"])\n",
        "    trainer.fit(model, train_loader, val_loader)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Xk9Z_Y1TvfUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3LoahT7R3ik0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "## https://github.com/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/custom-data-loader-celeba.ipynb\n",
        "import subprocess\n",
        "import os\n",
        "import abc\n",
        "import hashlib\n",
        "import zipfile\n",
        "import glob\n",
        "import logging\n",
        "import tarfile\n",
        "from skimage.io import imread\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, datasets\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class DisentangledDataset(Dataset, abc.ABC):\n",
        "    \"\"\"Base Class for disentangled VAE datasets.\n",
        "    Parameters\n",
        "    ----------\n",
        "    root : string\n",
        "        Root directory of dataset.\n",
        "    transforms_list : list\n",
        "        List of `torch.vision.transforms` to apply to the data when loading it.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, root, transforms_list=[], logger=logging.getLogger(__name__)):\n",
        "        self.root = root\n",
        "        self.train_data = os.path.join(root, type(self).files[\"train\"])\n",
        "        self.transforms = transforms.Compose(transforms_list)\n",
        "        self.logger = logger\n",
        "\n",
        "        if not os.path.isdir(root):\n",
        "            self.logger.info(\"Downloading {} ...\".format(str(type(self))))\n",
        "            self.download()\n",
        "            self.logger.info(\"Finished Downloading.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Get the image of `idx`.\n",
        "        Return\n",
        "        ------\n",
        "        sample : torch.Tensor\n",
        "            Tensor in [0.,1.] of shape `img_size`.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def download(self):\n",
        "        \"\"\"Download the dataset. \"\"\"\n",
        "        pass\n",
        "\n",
        "\n",
        "class CelebA(DisentangledDataset):\n",
        "    \"\"\"CelebA Dataset from [1].\n",
        "    CelebFaces Attributes Dataset (CelebA) is a large-scale face attributes dataset\n",
        "    with more than 200K celebrity images, each with 40 attribute annotations.\n",
        "    The images in this dataset cover large pose variations and background clutter.\n",
        "    CelebA has large diversities, large quantities, and rich annotations, including\n",
        "    10,177 number of identities, and 202,599 number of face images.\n",
        "    Notes\n",
        "    -----\n",
        "    - Link : http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html\n",
        "    Parameters\n",
        "    ----------\n",
        "    root : string\n",
        "        Root directory of dataset.\n",
        "    References\n",
        "    ----------\n",
        "    [1] Liu, Z., Luo, P., Wang, X., & Tang, X. (2015). Deep learning face\n",
        "        attributes in the wild. In Proceedings of the IEEE international conference\n",
        "        on computer vision (pp. 3730-3738).\n",
        "    \"\"\"\n",
        "    urls = {\"train\": \"https://s3-us-west-1.amazonaws.com/udacity-dlnfd/datasets/celeba.zip\"}\n",
        "    files = {\"train\": \"img_align_celeba\"}\n",
        "    img_size = (3, 64, 64)\n",
        "    background_color = COLOUR_WHITE\n",
        "\n",
        "    def __init__(self, root=os.path.join(DIR, '../data/celeba'), **kwargs):\n",
        "        super().__init__(root, [transforms.ToTensor()], **kwargs)\n",
        "\n",
        "        self.imgs = glob.glob(self.train_data + '/*')\n",
        "\n",
        "    def download(self):\n",
        "        \"\"\"Download the dataset.\"\"\"\n",
        "        save_path = os.path.join(self.root, 'celeba.zip')\n",
        "        os.makedirs(self.root)\n",
        "        subprocess.check_call([\"curl\", \"-L\", type(self).urls[\"train\"],\n",
        "                               \"--output\", save_path])\n",
        "\n",
        "        hash_code = '00d2c5bc6d35e252742224ab0c1e8fcb'\n",
        "        assert hashlib.md5(open(save_path, 'rb').read()).hexdigest() == hash_code, \\\n",
        "            '{} file is corrupted.  Remove the file and try again.'.format(save_path)\n",
        "\n",
        "        with zipfile.ZipFile(save_path) as zf:\n",
        "            self.logger.info(\"Extracting CelebA ...\")\n",
        "            zf.extractall(self.root)\n",
        "\n",
        "        os.remove(save_path)\n",
        "\n",
        "        self.logger.info(\"Resizing CelebA ...\")\n",
        "        preprocess(self.train_data, size=type(self).img_size[1:])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Get the image of `idx`\n",
        "        Return\n",
        "        ------\n",
        "        sample : torch.Tensor\n",
        "            Tensor in [0.,1.] of shape `img_size`.\n",
        "        placeholder :\n",
        "            Placeholder value as their are no targets.\n",
        "        \"\"\"\n",
        "        img_path = self.imgs[idx]\n",
        "        # img values already between 0 and 255\n",
        "        img = imread(img_path)\n",
        "\n",
        "        # put each pixel in [0.,1.] and reshape to (C x H x W)\n",
        "        img = self.transforms(img)\n",
        "\n",
        "        # no label so return 0 (note that can't return None because)\n",
        "        # dataloaders requires so\n",
        "        return img, 0\n",
        "\n",
        "\n",
        "# HELPERS\n",
        "def preprocess(root, size=(64, 64), img_format='JPEG', center_crop=None):\n",
        "    \"\"\"Preprocess a folder of images.\n",
        "    Parameters\n",
        "    ----------\n",
        "    root : string\n",
        "        Root directory of all images.\n",
        "    size : tuple of int\n",
        "        Size (width, height) to rescale the images. If `None` don't rescale.\n",
        "    img_format : string\n",
        "        Format to save the image in. Possible formats:\n",
        "        https://pillow.readthedocs.io/en/3.1.x/handbook/image-file-formats.html.\n",
        "    center_crop : tuple of int\n",
        "        Size (width, height) to center-crop the images. If `None` don't center-crop.\n",
        "    \"\"\"\n",
        "    imgs = []\n",
        "    for ext in [\".png\", \".jpg\", \".jpeg\"]:\n",
        "        imgs += glob.glob(os.path.join(root, '*' + ext))\n",
        "\n",
        "    for img_path in tqdm(imgs):\n",
        "        img = Image.open(img_path)\n",
        "        width, height = img.size\n",
        "\n",
        "        if size is not None and width != size[1] or height != size[0]:\n",
        "            img = img.resize(size, Image.ANTIALIAS)\n",
        "\n",
        "        if center_crop is not None:\n",
        "            new_width, new_height = center_crop\n",
        "            left = (width - new_width) // 2\n",
        "            top = (height - new_height) // 2\n",
        "            right = (width + new_width) // 2\n",
        "            bottom = (height + new_height) // 2\n",
        "\n",
        "            img.crop((left, top, right, bottom))\n",
        "\n",
        "        img.save(img_path, img_format)"
      ],
      "metadata": {
        "id": "LAGHL6S3D7du"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Module containing all vae losses.\n",
        "\"\"\"\n",
        "import abc\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch import optim\n",
        "\n",
        "#from disvae.utils.math import (log_density_gaussian, log_importance_weight_matrix,\n",
        "#                              matrix_log_density_gaussian)\n",
        "\n",
        "\n",
        "LOSSES = [\"VAE\", \"betaH\", \"betaB\", \"factor\", \"btcvae\"]\n",
        "RECON_DIST = [\"bernoulli\", \"laplace\", \"gaussian\"]\n",
        "\n",
        "class BaseLoss(abc.ABC):\n",
        "    \"\"\"\n",
        "    Base class for losses.\n",
        "    Parameters\n",
        "    ----------\n",
        "    record_loss_every: int, optional\n",
        "        Every how many steps to recorsd the loss.\n",
        "    rec_dist: {\"bernoulli\", \"gaussian\", \"laplace\"}, optional\n",
        "        Reconstruction distribution istribution of the likelihood on the each pixel.\n",
        "        Implicitely defines the reconstruction loss. Bernoulli corresponds to a\n",
        "        binary cross entropy (bse), Gaussian corresponds to MSE, Laplace\n",
        "        corresponds to L1.\n",
        "    steps_anneal: nool, optional\n",
        "        Number of annealing steps where gradually adding the regularisation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, record_loss_every=50, rec_dist=\"bernoulli\", steps_anneal=0):\n",
        "        self.n_train_steps = 0\n",
        "        self.record_loss_every = record_loss_every\n",
        "        self.rec_dist = rec_dist\n",
        "        self.steps_anneal = steps_anneal\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def __call__(self, data, recon_data, latent_dist, is_train, storer, **kwargs):\n",
        "        \"\"\"\n",
        "        Calculates loss for a batch of data.\n",
        "        Parameters\n",
        "        ----------\n",
        "        data : torch.Tensor\n",
        "            Input data (e.g. batch of images). Shape : (batch_size, n_chan,\n",
        "            height, width).\n",
        "        recon_data : torch.Tensor\n",
        "            Reconstructed data. Shape : (batch_size, n_chan, height, width).\n",
        "        latent_dist : tuple of torch.tensor\n",
        "            sufficient statistics of the latent dimension. E.g. for gaussian\n",
        "            (mean, log_var) each of shape : (batch_size, latent_dim).\n",
        "        is_train : bool\n",
        "            Whether currently in train mode.\n",
        "        storer : dict\n",
        "            Dictionary in which to store important variables for vizualisation.\n",
        "        kwargs:\n",
        "            Loss specific arguments\n",
        "        \"\"\"\n",
        "\n",
        "    def _pre_call(self, is_train, storer):\n",
        "        if is_train:\n",
        "            self.n_train_steps += 1\n",
        "\n",
        "        if not is_train or self.n_train_steps % self.record_loss_every == 1:\n",
        "            storer = storer\n",
        "        else:\n",
        "            storer = None\n",
        "\n",
        "        return storer\n",
        "\n",
        "\n",
        "\n",
        "class BetaHLoss(BaseLoss):\n",
        "    \"\"\"\n",
        "    Compute the Beta-VAE loss as in [1]\n",
        "    Parameters\n",
        "    ----------\n",
        "    beta : float, optional\n",
        "        Weight of the kl divergence.\n",
        "    kwargs:\n",
        "        Additional arguments for `BaseLoss`, e.g. rec_dist`.\n",
        "    References\n",
        "    ----------\n",
        "        [1] Higgins, Irina, et al. \"beta-vae: Learning basic visual concepts with\n",
        "        a constrained variational framework.\" (2016).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, beta=4, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.beta = beta\n",
        "\n",
        "    def __call__(self, data, recon_data, latent_dist, is_train, storer, **kwargs):\n",
        "        storer = self._pre_call(is_train, storer)\n",
        "\n",
        "        rec_loss = _reconstruction_loss(data, recon_data,\n",
        "                                        storer=storer,\n",
        "                                        distribution=self.rec_dist)\n",
        "        kl_loss = _kl_normal_loss(*latent_dist, storer)\n",
        "        anneal_reg = (linear_annealing(0, 1, self.n_train_steps, self.steps_anneal)\n",
        "                      if is_train else 1)\n",
        "        loss = rec_loss + anneal_reg * (self.beta * kl_loss)\n",
        "\n",
        "        if storer is not None:\n",
        "            storer['loss'].append(loss.item())\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "def _reconstruction_loss(data, recon_data, distribution=\"bernoulli\", storer=None):\n",
        "  \"\"\"\n",
        "  Calculates the per image reconstruction loss for a batch of data. I.e. negative\n",
        "  log likelihood.\n",
        "  Parameters\n",
        "  ----------\n",
        "  data : torch.Tensor\n",
        "      Input data (e.g. batch of images). Shape : (batch_size, n_chan,\n",
        "      height, width).\n",
        "  recon_data : torch.Tensor\n",
        "      Reconstructed data. Shape : (batch_size, n_chan, height, width).\n",
        "  distribution : {\"bernoulli\", \"gaussian\", \"laplace\"}\n",
        "      Distribution of the likelihood on the each pixel. Implicitely defines the\n",
        "      loss Bernoulli corresponds to a binary cross entropy (bse) loss and is the\n",
        "      most commonly used. It has the issue that it doesn't penalize the same\n",
        "      way (0.1,0.2) and (0.4,0.5), which might not be optimal. Gaussian\n",
        "      distribution corresponds to MSE, and is sometimes used, but hard to train\n",
        "      ecause it ends up focusing only a few pixels that are very wrong. Laplace\n",
        "      distribution corresponds to L1 solves partially the issue of MSE.\n",
        "  storer : dict\n",
        "      Dictionary in which to store important variables for vizualisation.\n",
        "  Returns\n",
        "  -------\n",
        "  loss : torch.Tensor\n",
        "      Per image cross entropy (i.e. normalized per batch but not pixel and\n",
        "      channel)\n",
        "  \"\"\"\n",
        "  batch_size, n_chan, height, width = recon_data.size()\n",
        "  is_colored = n_chan == 3\n",
        "\n",
        "  if distribution == \"bernoulli\":\n",
        "      loss = F.binary_cross_entropy(recon_data, data, reduction=\"sum\")\n",
        "  elif distribution == \"gaussian\":\n",
        "      # loss in [0,255] space but normalized by 255 to not be too big\n",
        "      loss = F.mse_loss(recon_data * 255, data * 255, reduction=\"sum\") / 255\n",
        "  elif distribution == \"laplace\":\n",
        "      # loss in [0,255] space but normalized by 255 to not be too big but\n",
        "      # multiply by 255 and divide 255, is the same as not doing anything for L1\n",
        "      loss = F.l1_loss(recon_data, data, reduction=\"sum\")\n",
        "      loss = loss * 3  # emperical value to give similar values than bernoulli => use same hyperparam\n",
        "      loss = loss * (loss != 0)  # masking to avoid nan\n",
        "  else:\n",
        "      assert distribution not in RECON_DIST\n",
        "      raise ValueError(\"Unkown distribution: {}\".format(distribution))\n",
        "\n",
        "  loss = loss / batch_size\n",
        "\n",
        "  if storer is not None:\n",
        "      storer['recon_loss'].append(loss.item())\n",
        "\n",
        "  return loss\n",
        "\n",
        "\n",
        "def _kl_normal_loss(mean, logvar, storer=None):\n",
        "  \"\"\"\n",
        "  Calculates the KL divergence between a normal distribution\n",
        "  with diagonal covariance and a unit normal distribution.\n",
        "  Parameters\n",
        "  ----------\n",
        "  mean : torch.Tensor\n",
        "      Mean of the normal distribution. Shape (batch_size, latent_dim) where\n",
        "      D is dimension of distribution.\n",
        "  logvar : torch.Tensor\n",
        "      Diagonal log variance of the normal distribution. Shape (batch_size,\n",
        "      latent_dim)\n",
        "  storer : dict\n",
        "      Dictionary in which to store important variables for vizualisation.\n",
        "  \"\"\"\n",
        "  latent_dim = mean.size(1)\n",
        "  # batch mean of kl for each latent dimension\n",
        "  latent_kl = 0.5 * (-1 - logvar + mean.pow(2) + logvar.exp()).mean(dim=0)\n",
        "  total_kl = latent_kl.sum()\n",
        "\n",
        "  if storer is not None:\n",
        "      storer['kl_loss'].append(total_kl.item())\n",
        "      for i in range(latent_dim):\n",
        "          storer['kl_loss_' + str(i)].append(latent_kl[i].item())\n",
        "\n",
        "  return total_kl \n",
        "\n",
        "def linear_annealing(init, fin, step, annealing_steps):\n",
        "    \"\"\"Linear annealing of a parameter.\"\"\"\n",
        "    if annealing_steps == 0:\n",
        "        return fin\n",
        "    assert fin > init\n",
        "    delta = fin - init\n",
        "    annealed = min(init + delta * step / annealing_steps, fin)\n",
        "    return annealed"
      ],
      "metadata": {
        "id": "z3ytnr7HJoq-"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z7grs2Q4PqGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "linear_annealing(0, 1, 10000000, 10000)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1H5z6TrqMriJ",
        "outputId": "d20fefad-59fc-400a-b633-7c6a8add4524"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import dataloader\n",
        "train_data = dataloader(celeba_data, batchsize=64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "id": "YQ5PLGXxAPfI",
        "outputId": "e6b60de2-4141-40b1-d5db-bf3f9e03e67d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-f4a594a90fc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mceleba_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch = next(iter(training_data_lr))"
      ],
      "metadata": {
        "id": "5B8SenBA4puG"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(batch[0].squeeze())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "8n-csMwd4wc9",
        "outputId": "2e1f50a9-34cb-4eaa-9b78-f691ae329c1b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f18a4cdea60>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}