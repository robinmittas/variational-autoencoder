## model parameters
latent_dimension: 32 # set to two for nice latent plot/ representations
hidden_dimensions: [32,64,128,256,512] # The hidden Dimensions basically specify the numer of convolutional Layers where each element of the list specifies the number of filters (e.g. the output dimension)
max_pool: [False,False,False,False,False] # needs to have the same length as hidden_dimension and specifies per block if we use max pool
kernel_size: [3,3]
stride: [2,2]
padding: 1
learning_rate: 0.001
train_valid_split: 0.9
epochs: 20
batch_size: 32
linear_layer_dimension: 2
last_conv_layer_kernel_size: [1,1]

# data params
data_path: "..\\torch_vae\\data\\celeba" # for linux user: "../torch_vae/data/celeba"
resize: 64
input_image_size: [3,64,64] # CELEBA Dataset resize to 3x64x64 due to memory issues.


## loss specifications
KL_divergence_weight: 0.001 # https://github.com/ramachandran-lab/HEBAE/blob/master/CelebA/CelebA_HEBAE_train.py
mse_reduction: "sum" # possible values: sum or mean (e.g. pixelwise summed loss or average loss)


# Some parameters what we want to log:
plot_2_interpolate_dir: "./plots/hebea_vae_interpolate_2_numbers.png"
plot_sample: "./plots/hebea_vae_sampled.png"
accelerator: "gpu"
devices: 1
logging_dir: "./logs"
logging_name: "HEBEA"
